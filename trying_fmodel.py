# -*- coding: utf-8 -*-
"""Trying FModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-KTX3Zw5oWNRrKpDYiFlNZLTgStYPmq6
"""

# First, install all required packages
import subprocess
import sys

def install_dependencies():
    """Install all required packages"""
    packages = [
        "groq>=0.4.0",
        "langchain>=0.1.0",
        "langchain-community>=0.0.20",
        "faiss-cpu>=1.7.4",
        "sentence-transformers>=2.2.2",
        "PyMuPDF>=1.23.0",
        "openpyxl>=3.1.0",
        "pandas>=2.0.0",
        "pydantic>=2.0.0",
        "tiktoken>=0.5.0",
        "numpy>=1.24.0"
    ]

    print("üîß Installing dependencies...")
    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"‚úÖ Installed {package}")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to install {package}: {e}")

    print("‚úÖ All dependencies installed!")

install_dependencies()

import os
import json
import re
import time
from typing import List, Dict, Optional, Tuple, Any
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Core libraries
import pandas as pd
import numpy as np

# PDF processing
import fitz  # PyMuPDF

# ML/NLP libraries
from sentence_transformers import SentenceTransformer
import tiktoken

# LangChain components
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

# Groq API
from groq import Groq

# Excel export
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.styles import Font, Alignment, numbers

# JSON validation
from pydantic import BaseModel, RootModel, ValidationError

# ============================================================================
# CONFIGURATION & CONSTANTS
# ============================================================================

# API Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "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")  # Set your API key here or in environment
GROQ_MODEL = "llama-3.3-70b-versatile"

# Model configurations
EMBED_MODEL_NAME = "all-MiniLM-L6-v2"

# RAG parameters
CHUNK_SIZE = 400
CHUNK_OVERLAP = 80
MAX_CONTEXT_TOKENS = 3000
TOP_K_CANDIDATES = 10
TOP_N_RESULTS = 7

# Financial statement headers to detect
CORE_STATEMENT_HEADERS = [
    "Consolidated Statements of Operations",
    "Consolidated Balance Sheets",
    "Consolidated Statements of Cash Flows"
]

# Section boost weights for hybrid search
SECTION_BOOST_WEIGHTS = {
    "Consolidated Statements of Operations": 2.0,
    "Consolidated Balance Sheets": 2.0,
    "Consolidated Statements of Cash Flows": 2.0,
    "other": 1.0
}

# ============================================================================
# PHASE 0: SETUP & VERIFICATION
# ============================================================================

def setup_groq_client() -> Groq:
    """Initialize and verify Groq client"""
    if not GROQ_API_KEY:
        raise ValueError(
            "GROQ_API_KEY not set! Please set it in the script or environment:\n"
            "os.environ['GROQ_API_KEY'] = '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'"
        )

    try:
        client = Groq(api_key=GROQ_API_KEY)
        # Test connection
        test_response = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": "Hello"}],
            max_tokens=10,
            temperature=0
        )
        print("‚úÖ Groq API connection verified")
        return client
    except Exception as e:
        raise ValueError(f"Groq API setup failed: {e}")

def extract_and_clean_text(pdf_path: str) -> str:
    """
    Extract and clean text from PDF using PyMuPDF.

    Args:
        pdf_path: Path to PDF file

    Returns:
        Cleaned text as single string
    """
    try:
        doc = fitz.open(pdf_path)
        pages = []

        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text = page.get_text()

            # Basic cleaning
            lines = [line.strip() for line in text.split("\n") if line.strip()]
            # Remove lines that are just page numbers
            lines = [line for line in lines if not line.isdigit()]
            # Remove common headers/footers (simple heuristic)
            lines = [line for line in lines if len(line) > 3]

            if lines:
                pages.append(" ".join(lines))

        doc.close()
        full_text = "\n\n".join(pages)

        # Final cleanup
        full_text = re.sub(r'\n{3,}', '\n\n', full_text)
        full_text = re.sub(r'\s+', ' ', full_text)

        print(f"‚úÖ Extracted {len(full_text)} characters from {pdf_path}")
        return full_text

    except Exception as e:
        print(f"‚ùå Error extracting text from {pdf_path}: {e}")
        return ""

def split_by_headings(text: str, headings: List[str]) -> List[str]:
    """
    Split text by financial statement headings.

    Args:
        text: Full document text
        headings: List of heading patterns to split on

    Returns:
        List of text sections
    """
    # Create regex pattern for all headings
    pattern = "(" + "|".join(map(re.escape, headings)) + ")"
    parts = re.split(pattern, text, flags=re.IGNORECASE)

    sections = []
    if parts:
        sections.append(parts[0])  # Text before first heading

        # Combine headings with their content
        for i in range(1, len(parts), 2):
            if i + 1 < len(parts):
                heading = parts[i]
                content = parts[i + 1]
                sections.append(heading + "\n" + content)
            else:
                sections.append(parts[i])

    return sections

def intelligent_chunking(text: str) -> List[Document]:
    """
    Break text into chunks with section type tagging.

    Args:
        text: Full document text

    Returns:
        List of Document objects with metadata
    """
    sections = split_by_headings(text, CORE_STATEMENT_HEADERS)
    splitter = CharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separator="\n"
    )

    docs = []

    for section in sections:
        if not section.strip():
            continue

        # Determine section type
        section_type = "other"
        for header in CORE_STATEMENT_HEADERS:
            if header.lower() in section.lower()[:200]:  # Check first 200 chars
                section_type = header
                break

        # Split section into chunks
        chunks = splitter.split_text(section)

        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) > 50:  # Skip very short chunks
                docs.append(Document(
                    page_content=chunk,
                    metadata={
                        "section_type": section_type,
                        "chunk_id": i,
                        "section_preview": section[:100] + "..." if len(section) > 100 else section
                    }
                ))

    print(f"üìÑ Created {len(docs)} chunks from {len(sections)} sections")
    return docs

def build_vector_store(docs: List[Document]) -> FAISS:
    """
    Build FAISS vector store from documents.

    Args:
        docs: List of Document objects

    Returns:
        FAISS vector store
    """
    embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)

    if not docs:
        raise ValueError("No documents provided for vector store")

    vector_store = FAISS.from_documents(docs, embedder)
    print(f"üîç Built FAISS index with {len(docs)} documents")
    return vector_store

def hybrid_search(query: str, vector_store: FAISS, k: int = TOP_K_CANDIDATES, top_n: int = TOP_N_RESULTS) -> List[Document]:
    """
    Perform hybrid search with section boosting.

    Args:
        query: Search query
        vector_store: FAISS vector store
        k: Number of candidates to retrieve
        top_n: Number of final results to return

    Returns:
        List of top-ranked documents
    """
    # Get semantic similarity candidates
    candidates = vector_store.similarity_search_with_score(query, k=k)

    # Apply section boosting
    boosted_candidates = []
    for doc, score in candidates:
        section_type = doc.metadata.get("section_type", "other")
        boost = SECTION_BOOST_WEIGHTS.get(section_type, 1.0)
        boosted_score = score * boost
        boosted_candidates.append((doc, boosted_score))

    # Sort by boosted score (lower is better for similarity)
    boosted_candidates.sort(key=lambda x: x[1])

    # Return top N documents
    return [doc for doc, _ in boosted_candidates[:top_n]]

def count_tokens(text: str, encoding_name: str = "gpt2") -> int:
    """Count tokens in text using tiktoken"""
    try:
        enc = tiktoken.get_encoding(encoding_name)
        return len(enc.encode(text))
    except Exception:
        # Fallback to rough estimation
        return len(text.split()) * 1.3

def build_rag_prompt(question: str, docs: List[Document], max_tokens: int = MAX_CONTEXT_TOKENS) -> str:
    """
    Build RAG prompt with token budget management.

    Args:
        question: User question
        docs: Retrieved documents
        max_tokens: Maximum token budget

    Returns:
        Complete prompt string
    """
    # System prompt
    header = (
        "You are a top financial analyst. Extract EXACT numbers from the provided "
        "10-K filing excerpts and return them in strict JSON format.\n\n"
        "IMPORTANT: Only use numbers explicitly stated in the context. "
        "If a value is not found, return 'Not found' for that year.\n\n"
    )

    prompt = header
    tokens_used = count_tokens(prompt)

    # Separate core and other documents
    core_docs = [d for d in docs if d.metadata.get("section_type") != "other"]
    other_docs = [d for d in docs if d.metadata.get("section_type") == "other"]

    # Add core documents first (up to 5)
    chunks_added = 0
    for doc in core_docs[:5]:
        section_type = doc.metadata.get("section_type", "unknown")
        chunk_text = f"--- {section_type} ---\n{doc.page_content}\n\n"

        chunk_tokens = count_tokens(chunk_text)
        if tokens_used + chunk_tokens > max_tokens - 200:  # Reserve space for question
            break

        prompt += chunk_text
        tokens_used += chunk_tokens
        chunks_added += 1

    # Add other documents (up to 2)
    for doc in other_docs[:2]:
        chunk_text = f"--- Other Context ---\n{doc.page_content}\n\n"

        chunk_tokens = count_tokens(chunk_text)
        if tokens_used + chunk_tokens > max_tokens - 200:
            break

        prompt += chunk_text
        tokens_used += chunk_tokens
        chunks_added += 1

    # Add question
    question_block = f"QUESTION: {question}\n\nReturn JSON in this exact format:\n"
    question_block += '{"MetricName": {"2022": "value", "2023": "value", "2024": "value"}}\n\n'
    question_block += "JSON Response:"

    prompt += question_block

    print(f"üìù Built prompt with {chunks_added} chunks, ~{count_tokens(prompt)} tokens")
    return prompt

def call_groq_llama(prompt: str, groq_client: Groq) -> str:
    """
    Call Groq LLaMA API with error handling.

    Args:
        prompt: Input prompt
        groq_client: Groq client instance

    Returns:
        LLM response text
    """
    try:
        response = groq_client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=512,
            top_p=0.9
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        print(f"‚ùå Groq API call failed: {e}")
        return f'{{"error": "API call failed: {str(e)}"}}'

def extract_json_from_response(response_text: str) -> str:
    """Extract JSON from LLM response"""
    # Look for JSON block
    json_match = re.search(r'\{[^{}]*\{[^{}]*\}[^{}]*\}|\{[^{}]*\}', response_text, re.DOTALL)
    if json_match:
        return json_match.group(0)

    # Fallback: find first { to last }
    start = response_text.find('{')
    end = response_text.rfind('}')
    if start != -1 and end != -1 and end > start:
        return response_text[start:end+1]

    raise ValueError("No JSON found in response")

def parse_json_response(response_text: str) -> Dict[str, Dict[str, str]]:
    """
    Parse and validate JSON response from LLM.

    Args:
        response_text: Raw LLM response

    Returns:
        Parsed JSON data
    """
    try:
        json_str = extract_json_from_response(response_text)
        data = json.loads(json_str)

        # Validate structure
        if not isinstance(data, dict):
            raise ValueError("Response is not a JSON object")

        # Ensure all values are dictionaries with string keys and values
        validated_data = {}
        for metric, year_data in data.items():
            if isinstance(year_data, dict):
                validated_data[metric] = {str(k): str(v) for k, v in year_data.items()}
            else:
                validated_data[metric] = {"error": "Invalid data format"}

        return validated_data

    except Exception as e:
        print(f"‚ùå JSON parsing error: {e}")
        return {"error": f"Parse failed: {str(e)}"}

def safe_call_and_parse(question: str, docs: List[Document], groq_client: Groq, max_retries: int = 2) -> Dict[str, Dict[str, str]]:
    """
    Safely call LLM and parse response with retries.

    Args:
        question: User question
        docs: Retrieved documents
        groq_client: Groq client instance
        max_retries: Maximum retry attempts

    Returns:
        Parsed financial data
    """
    for attempt in range(max_retries):
        try:
            prompt = build_rag_prompt(question, docs)
            response = call_groq_llama(prompt, groq_client)

            if not response or response.strip() == "":
                raise ValueError("Empty response from LLM")

            parsed_data = parse_json_response(response)

            if "error" not in parsed_data:
                return parsed_data
            else:
                raise ValueError(f"LLM returned error: {parsed_data['error']}")

        except Exception as e:
            print(f"‚ùå Attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(1)
            else:
                return {"error": f"All attempts failed: {str(e)}"}

# ============================================================================
# PHASE 5: FINANCIAL DATA EXTRACTION PIPELINE
# ============================================================================

def extract_financial_data(pdf_paths: List[str], metrics: List[str], groq_client: Groq) -> Dict[str, Dict[str, str]]:
    """
    Main extraction pipeline for financial data.

    Args:
        pdf_paths: List of PDF file paths
        metrics: List of financial metrics to extract
        groq_client: Groq client instance

    Returns:
        Dictionary of extracted financial data
    """
    print("üöÄ Starting financial data extraction pipeline...")

    # Step 1: Extract and combine text from all PDFs
    all_text = []
    for pdf_path in pdf_paths:
        text = extract_and_clean_text(pdf_path)
        if text:
            all_text.append(text)

    if not all_text:
        raise ValueError("No text extracted from any PDF files")

    combined_text = "\n\n".join(all_text)

    # Step 2: Intelligent chunking
    docs = intelligent_chunking(combined_text)

    # Step 3: Build vector store
    vector_store = build_vector_store(docs)

    # Step 4: Extract each metric
    results = {}
    total_metrics = len(metrics)

    for i, metric in enumerate(metrics):
        print(f"\nüìä Extracting {metric} ({i+1}/{total_metrics})...")

        # Build query
        query = f"What was the {metric} for fiscal years 2022, 2023, and 2024?"

        # Retrieve relevant documents
        relevant_docs = hybrid_search(query, vector_store)

        # Extract data using LLM
        extracted_data = safe_call_and_parse(query, relevant_docs, groq_client)

        # Store results
        if extracted_data and "error" not in extracted_data:
            # Find the metric in the response (case-insensitive)
            found_metric = None
            for key in extracted_data.keys():
                if key.lower().replace(" ", "").replace("_", "") == metric.lower().replace(" ", "").replace("_", ""):
                    found_metric = key
                    break

            if found_metric:
                results[metric] = extracted_data[found_metric]
                print(f"‚úÖ Successfully extracted {metric}")
            else:
                # Take the first non-error result
                for key, value in extracted_data.items():
                    if isinstance(value, dict) and "error" not in value:
                        results[metric] = value
                        print(f"‚úÖ Extracted {metric} (mapped from {key})")
                        break
                else:
                    results[metric] = {"error": "No valid data found"}
                    print(f"‚ùå Failed to extract {metric}")
        else:
            results[metric] = extracted_data
            print(f"‚ùå Failed to extract {metric}")

    return results

# ============================================================================
# PHASE 6: DATAFRAME CONSTRUCTION & MODELING
# ============================================================================

def build_statement_dfs(extracted_data: Dict[str, Dict[str, str]]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Build three financial statement DataFrames.

    Args:
        extracted_data: Dictionary of extracted financial data

    Returns:
        Tuple of (income_df, balance_df, cashflow_df)
    """
    # Define line items for each statement
    income_items = [
        "Total Revenue", "Cost of Goods Sold", "Gross Profit",
        "Operating Expenses", "Operating Income", "Interest Expense",
        "Tax Expense", "Net Income"
    ]

    balance_items = [
        "Cash and Cash Equivalents", "Accounts Receivable", "Inventory",
        "Property, Plant & Equipment", "Total Assets", "Accounts Payable",
        "Total Debt", "Shareholders' Equity"
    ]

    cashflow_items = [
        "Operating Cash Flow", "Capital Expenditures", "Free Cash Flow",
        "Financing Cash Flow"
    ]

    # Get all years from the data
    all_years = set()
    for metric_data in extracted_data.values():
        if isinstance(metric_data, dict) and "error" not in metric_data:
            for year in metric_data.keys():
                try:
                    all_years.add(int(year))
                except ValueError:
                    continue

    years = sorted(all_years) if all_years else [2022, 2023, 2024]

    def create_df(items: List[str]) -> pd.DataFrame:
        """Create DataFrame for a statement"""
        df = pd.DataFrame(index=items, columns=years, dtype=float)

        for item in items:
            if item in extracted_data:
                year_data = extracted_data[item]
                if isinstance(year_data, dict) and "error" not in year_data:
                    for year_str, value_str in year_data.items():
                        try:
                            year = int(year_str)
                            # Clean and convert value
                            value = str(value_str).replace(",", "").replace("$", "").replace("(", "-").replace(")", "")
                            # Handle "Not found" or similar
                            if "not found" in value.lower() or "n/a" in value.lower():
                                df.at[item, year] = np.nan
                            else:
                                df.at[item, year] = float(value)
                        except (ValueError, TypeError):
                            df.at[item, year] = np.nan

        return df

    income_df = create_df(income_items)
    balance_df = create_df(balance_items)
    cashflow_df = create_df(cashflow_items)

    print(f"üìä Created DataFrames for {len(years)} years: {years}")
    return income_df, balance_df, cashflow_df

def add_retained_earnings(income_df: pd.DataFrame, balance_df: pd.DataFrame) -> pd.DataFrame:
    """
    Add retained earnings calculation to balance sheet.

    Args:
        income_df: Income statement DataFrame
        balance_df: Balance sheet DataFrame

    Returns:
        Updated balance sheet DataFrame
    """
    balance_df = balance_df.copy()

    if "Net Income" in income_df.index:
        # Calculate cumulative retained earnings
        net_income = income_df.loc["Net Income"].fillna(0)
        retained_earnings = net_income.cumsum()

        # Add to balance sheet
        balance_df.loc["Retained Earnings"] = retained_earnings

        # Reorder to put after Shareholders' Equity if it exists
        if "Shareholders' Equity" in balance_df.index:
            rows = balance_df.index.tolist()
            re_idx = rows.index("Retained Earnings")
            se_idx = rows.index("Shareholders' Equity")

            if re_idx < se_idx:
                rows.insert(se_idx + 1, rows.pop(re_idx))
                balance_df = balance_df.reindex(rows)

    return balance_df

def forecast_revenue(income_df: pd.DataFrame, growth_rate: Optional[float] = None) -> pd.DataFrame:
    """
    Add revenue forecast for next year.

    Args:
        income_df: Income statement DataFrame
        growth_rate: Growth rate to use (if None, calculate from historical data)

    Returns:
        Updated income statement DataFrame with forecast
    """
    income_df = income_df.copy()

    if "Total Revenue" not in income_df.index:
        return income_df

    revenue = income_df.loc["Total Revenue"].dropna()

    if len(revenue) < 2:
        return income_df

    # Calculate growth rate if not provided
    if growth_rate is None:
        growth_rates = revenue.pct_change().dropna()
        growth_rate = growth_rates.mean() if len(growth_rates) > 0 else 0.05

    # Forecast next year
    last_year = revenue.index[-1]
    next_year = last_year + 1
    forecast_revenue = revenue.iloc[-1] * (1 + growth_rate)

    income_df.loc["Total Revenue", next_year] = forecast_revenue

    print(f"üìà Added revenue forecast for {next_year}: {forecast_revenue:,.0f} (growth: {growth_rate:.1%})")
    return income_df

# ============================================================================
# PHASE 7: EXCEL EXPORT
# ============================================================================

def create_financial_model_excel(income_df: pd.DataFrame, balance_df: pd.DataFrame,
                                cashflow_df: pd.DataFrame, filename: str = "financial_model.xlsx"):
    """
    Export financial model to Excel with professional formatting.

    Args:
        income_df: Income statement DataFrame
        balance_df: Balance sheet DataFrame
        cashflow_df: Cash flow statement DataFrame
        filename: Output filename
    """
    wb = Workbook()

    # Define sheets and their data
    sheets_data = [
        ("Income Statement", income_df),
        ("Balance Sheet", balance_df),
        ("Cash Flow", cashflow_df)
    ]

    for idx, (sheet_name, df) in enumerate(sheets_data):
        # Use existing sheet for first one, create new for others
        if idx == 0:
            ws = wb.active
            ws.title = sheet_name
        else:
            ws = wb.create_sheet(sheet_name)

        # Write data to sheet
        for r_idx, row in enumerate(dataframe_to_rows(df, index=True, header=True), 1):
            for c_idx, value in enumerate(row, 1):
                cell = ws.cell(row=r_idx, column=c_idx, value=value)

                # Format headers
                if r_idx == 1 or c_idx == 1:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center")

                # Format numbers
                elif isinstance(value, (int, float)) and not pd.isna(value):
                    cell.number_format = '#,##0'
                    if abs(value) >= 1000000:
                        cell.number_format = '#,##0,,"M"'  # Millions

        # Auto-fit columns
        for column in ws.columns:
            max_length = 0
            column_letter = column[0].column_letter

            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass

            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column_letter].width = adjusted_width

    # Save workbook
    wb.save(filename)
    print(f"üìä Excel file saved: {filename}")

# ============================================================================
# MAIN EXECUTION FUNCTION
# ============================================================================

def main():
    """Main execution function"""
    print("="*80)
    print("üöÄ FINANCIAL RAG PIPELINE - COMPLETE EXECUTION")
    print("="*80)

    # Configuration
    PDF_PATHS = [
        "SEC_Apple_10k.pdf",
        "Q1-2025 Transcript AAPL.pdf"
    ]

    METRICS_TO_EXTRACT = [
        # Income Statement
        "Total Revenue", "Cost of Goods Sold", "Gross Profit",
        "Operating Expenses", "Operating Income", "Interest Expense",
        "Tax Expense", "Net Income",

        # Balance Sheet
        "Cash and Cash Equivalents", "Accounts Receivable", "Inventory",
        "Property, Plant & Equipment", "Total Assets", "Accounts Payable",
        "Total Debt", "Shareholders' Equity",

        # Cash Flow Statement
        "Operating Cash Flow", "Capital Expenditures", "Free Cash Flow",
        "Financing Cash Flow"
    ]

    try:
        # Phase 0: Setup
        print("\nüîß Phase 0: Setting up environment...")
        groq_client = setup_groq_client()

        # Phase 1-5: Extract financial data
        print("\nüìä Phase 1-5: Extracting financial data...")
        extracted_data = extract_financial_data(PDF_PATHS, METRICS_TO_EXTRACT, groq_client)

        # Phase 6: Build DataFrames
        print("\nüìà Phase 6: Building financial statements...")
        income_df, balance_df, cashflow_df = build_statement_dfs(extracted_data)

        # Add retained earnings
        balance_df = add_retained_earnings(income_df, balance_df)

        # Add revenue forecast
        income_df = forecast_revenue(income_df)

        # Phase 7: Export to Excel
        print("\nüíæ Phase 7: Exporting to Excel...")
        create_financial_model_excel(income_df, balance_df, cashflow_df)

        # Display summary
        print("\n" + "="*80)
        print("üìä EXTRACTION SUMMARY")
        print("="*80)

        successful_extractions = 0
        for metric, data in extracted_data.items():
            if isinstance(data, dict) and "error" not in data:
                successful_extractions += 1
                print(f"‚úÖ {metric}: {data}")
            else:
                print(f"‚ùå {metric}: {data}")

        print(f"\nüéØ Successfully extracted {successful_extractions}/{len(METRICS_TO_EXTRACT)} metrics")

        print("\nüìä FINANCIAL STATEMENTS PREVIEW")
        print("="*50)

        print("\nüí∞ INCOME STATEMENT")
        print(income_df.head(8))

        print("\nüíº BALANCE SHEET")
        print(balance_df.head(8))

        print("\nüí∏ CASH FLOW STATEMENT")
        print(cashflow_df.head(4))

        print("\n‚úÖ Pipeline completed successfully!")
        print("üìÅ Check 'financial_model.xlsx' for full results")

    except Exception as e:
        print(f"\n‚ùå Pipeline failed: {e}")
        import traceback
        traceback.print_exc()

# ============================================================================
# UTILITY FUNCTIONS FOR STANDALONE USAGE
# ============================================================================

def quick_extract(pdf_path: str, metric: str, api_key: str = None) -> Dict[str, str]:
    """
    Quick extraction of a single metric from one PDF.

    Args:
        pdf_path: Path to PDF file
        metric: Financial metric to extract
        api_key: Groq API key (optional if set in environment)

    Returns:
        Dictionary with years as keys and values as strings
    """
    if api_key:
        os.environ["GROQ_API_KEY"] = api_key

    try:
        groq_client = setup_groq_client()
        result = extract_financial_data([pdf_path], [metric], groq_client)
        return result.get(metric, {"error": "Metric not found"})
    except Exception as e:
        return {"error": str(e)}

def batch_process_pdfs(pdf_directory: str, output_filename: str = "batch_results.xlsx") -> None:
    """
    Process all PDFs in a directory and create combined financial model.

    Args:
        pdf_directory: Directory containing PDF files
        output_filename: Output Excel filename
    """
    import glob

    pdf_files = glob.glob(os.path.join(pdf_directory, "*.pdf"))

    if not pdf_files:
        print(f"‚ùå No PDF files found in {pdf_directory}")
        return

    print(f"üìÅ Found {len(pdf_files)} PDF files")

    try:
        groq_client = setup_groq_client()

        # Standard metrics for batch processing
        standard_metrics = [
            "Total Revenue", "Net Income", "Total Assets",
            "Shareholders' Equity", "Operating Cash Flow"
        ]

        extracted_data = extract_financial_data(pdf_files, standard_metrics, groq_client)
        income_df, balance_df, cashflow_df = build_statement_dfs(extracted_data)

        create_financial_model_excel(income_df, balance_df, cashflow_df, output_filename)
        print(f"‚úÖ Batch processing complete: {output_filename}")

    except Exception as e:
        print(f"‚ùå Batch processing failed: {e}")

def validate_extraction_quality(extracted_data: Dict[str, Dict[str, str]]) -> Dict[str, float]:
    """
    Validate the quality of extracted financial data.

    Args:
        extracted_data: Dictionary of extracted financial data

    Returns:
        Dictionary with quality metrics
    """
    total_metrics = len(extracted_data)
    successful_extractions = 0
    total_data_points = 0
    successful_data_points = 0

    for metric, year_data in extracted_data.items():
        if isinstance(year_data, dict) and "error" not in year_data:
            successful_extractions += 1

            for year, value in year_data.items():
                total_data_points += 1
                if value and "not found" not in str(value).lower():
                    successful_data_points += 1

    return {
        "metric_success_rate": successful_extractions / total_metrics if total_metrics > 0 else 0,
        "data_point_success_rate": successful_data_points / total_data_points if total_data_points > 0 else 0,
        "total_metrics": total_metrics,
        "successful_metrics": successful_extractions,
        "total_data_points": total_data_points,
        "successful_data_points": successful_data_points
    }

# ============================================================================
# EXAMPLE USAGE & TESTING
# ============================================================================

def run_example():
    """Example usage of the pipeline"""
    print("üß™ Running example extraction...")

    # Example for single metric extraction
    result = quick_extract("example_10k.pdf", "Total Revenue")
    print(f"Quick extract result: {result}")

    # Example quality validation
    sample_data = {
        "Revenue": {"2022": "100000", "2023": "110000", "2024": "120000"},
        "Net Income": {"2022": "10000", "2023": "Not found", "2024": "15000"},
        "Assets": {"error": "Extraction failed"}
    }

    quality = validate_extraction_quality(sample_data)
    print(f"Quality metrics: {quality}")

# ============================================================================
# SCRIPT EXECUTION
# ============================================================================

if __name__ == "__main__":
    # Set your Groq API key here if not in environment
    # os.environ["GROQ_API_KEY"] = "your_api_key_here"

    # Run the main pipeline
    main()

    # Uncomment to run example
    # run_example()

    print("\n" + "="*80)
    print("üéâ FINANCIAL RAG PIPELINE COMPLETE!")
    print("="*80)

